{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1f3fc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor(object):\n",
    "\n",
    "    \n",
    "    def __init__(self, data,\n",
    "                 autograd=False,\n",
    "                 creators=None,\n",
    "                 creation_op=None,\n",
    "                 id=None):\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        \n",
    "        if id is None:\n",
    "            id = np.random.randint(0, 100000)\n",
    "        self.id = id\n",
    "        \n",
    "        if creators is not None:\n",
    "            for c in creators:\n",
    "                if self.id not in c.children:\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "                    \n",
    "                    \n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id, cnt in self.children.items():\n",
    "            if cnt != 0:\n",
    "                return False\n",
    "        return True\n",
    "        \n",
    "        \n",
    "    def backward(self, grad=None, grad_origin=None):\n",
    "        if self.autograd:\n",
    "            if grad is None:\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "            \n",
    "            if grad_origin is not None:\n",
    "                if self.children[grad_origin.id] == 0:\n",
    "                    raise Exception('cannot backprop more than once')\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "            \n",
    "            if self.grad is None:\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "            \n",
    "            if self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None):\n",
    "                if self.creation_op == 'add':\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                \n",
    "                if self.creation_op == 'neg':\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                    \n",
    "                if self.creation_op == 'sub':\n",
    "                    new = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = Tensor(self.grad.__neg__().data)\n",
    "                    self.creators[1].backward(new, self)\n",
    "                \n",
    "                if self.creation_op == 'mul':\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)\n",
    "                    \n",
    "                if self.creation_op == 'mm':\n",
    "                    act = self.creators[0]     # usually an activation\n",
    "                    weights = self.creators[1] # usually a weight matrix\n",
    "                    new = self.grad.mm(weights.transpose())\n",
    "                    act.backward(new)\n",
    "                    new = self.grad.transpose().mm(act).transpose()\n",
    "                    weights.backward(new)\n",
    "\n",
    "                if self.creation_op == 'transpose':\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "                    \n",
    "                if 'sum' in self.creation_op:\n",
    "                    dim = int(self.creation_op.split('_')[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim,ds))\n",
    "                    \n",
    "                if 'expand' in self.creation_op:\n",
    "                    dim = int(self.creation_op.split('_')[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                    \n",
    "                if self.creation_op == 'sigmoid':\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                    \n",
    "                if self.creation_op == 'tanh':\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "    \n",
    "                if self.creation_op == 'index_select':\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "\n",
    "                if self.creation_op == 'cross_entropy':\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "\n",
    "\n",
    "    def __add__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data + other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self, other],\n",
    "                          creation_op='add')\n",
    "        return Tensor(self.data + other.data)\n",
    "\n",
    "    \n",
    "    def __neg__(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data * -1,\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op='neg')\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "   \n",
    "    def __sub__(self, other):\n",
    "        if self.autograd and other.autograd: \n",
    "            return Tensor(self.data - other.data, \n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if self.autograd and other.autograd:\n",
    "            return Tensor(self.data * other.data,\n",
    "                          autograd=True,\n",
    "                          creators=[self,other],\n",
    "                          creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "\n",
    "    \n",
    "    def sum(self, dim):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.sum(dim),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "\n",
    "    \n",
    "    def expand(self, dim, copies):\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        if self.autograd:\n",
    "            return Tensor(new_data, autograd=True, creators=[self], creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "\n",
    "    \n",
    "    def transpose(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.transpose(),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op=\"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "\n",
    "    \n",
    "    def mm(self, x):\n",
    "        if self.autograd:\n",
    "            return Tensor(self.data.dot(x.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self,x],\n",
    "                          creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op='sigmoid')\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "    \n",
    "    \n",
    "    def tanh(self):\n",
    "        if self.autograd:\n",
    "            return Tensor(np.tanh(self.data),\n",
    "                          autograd=True,\n",
    "                          creators=[self],\n",
    "                          creation_op='tanh')\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    \n",
    "    def softmax(self):\n",
    "        e_x = np.exp(self.data - np.max(self.data))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "        if self.autograd:\n",
    "            new = Tensor(self.data[indices.data],\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op='index_select')\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "                                              \n",
    "                                              \n",
    "    def cross_entropy(self, target_indices):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "                                       axis=len(self.data.shape)-1,\n",
    "                                       keepdims=True)\n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t),-1)\n",
    "    \n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "        \n",
    "        if self.autograd:\n",
    "            out = Tensor(loss,\n",
    "                         autograd=True,\n",
    "                         creators=[self],\n",
    "                         creation_op='cross_entropy')\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "        return Tensor(loss)                                              \n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "\n",
    "\n",
    "# Stochastic gradient descent\n",
    "\n",
    "class SGD(object):\n",
    "\n",
    "    \n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    \n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "            \n",
    "    \n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            \n",
    "            if zero:\n",
    "                p.grad.data *= 0\n",
    "               \n",
    "\n",
    "# Layers\n",
    "            \n",
    "class Layer(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "        \n",
    "    \n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "    \n",
    "\n",
    "class Linear(Layer):\n",
    "\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
    "        super().__init__()\n",
    "        W = np.random.randn(n_inputs, n_outputs) * np.sqrt(2.0 / n_inputs)\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.parameters.append(self.weight)\n",
    "        if bias:\n",
    "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "            self.parameters.append(self.bias)\n",
    "        else:\n",
    "            self.bias = None\n",
    "    \n",
    "    \n",
    "    def forward(self, input):\n",
    "        temp = input.mm(self.weight)\n",
    "        if self.bias:\n",
    "            temp += self.bias.expand(0, len(input.data))\n",
    "        return temp\n",
    "                \n",
    "\n",
    "class Sequential(Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self, layers=list()):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        \n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "    \n",
    "    \n",
    "    def get_parameters(self):\n",
    "        params = list()\n",
    "        for l in self.layers:\n",
    "            params += l.get_parameters()\n",
    "        return params        \n",
    "        \n",
    "        \n",
    "class Embedding(Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        \n",
    "        weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
    "        self.weight = Tensor(weight, autograd=True)\n",
    "        \n",
    "        self.parameters.append(self.weight)\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        return self.weight.index_select(input)\n",
    "\n",
    "\n",
    "# Activate functions    \n",
    "    \n",
    "class Tanh(Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        return input.tanh()\n",
    "    \n",
    "    \n",
    "class Sigmoid(Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "    \n",
    "\n",
    "class CrossEntropyLoss(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)    \n",
    "    \n",
    "\n",
    "# Recurrent cell\n",
    "    \n",
    "class RNNCell(Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_inputs, n_hidden, n_output, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = Sigmoid()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = Tanh()\n",
    "        else:\n",
    "            raise Exception('unknown non-linear function')\n",
    "        \n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "        \n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "\n",
    "\n",
    "# Error\n",
    "                \n",
    "class MSELoss(Layer):\n",
    "    \n",
    "    \n",
    "    def __init(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        return ((pred - target)*(pred - target)).sum(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369013d7",
   "metadata": {},
   "source": [
    "# start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1b244fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "with open('shakespear.txt') as f:\n",
    "    raw = f.read()\n",
    "vocab = list(set(raw))\n",
    "word2index = {word:i for i,word in enumerate(vocab)}\n",
    "indices = np.array(list(map(lambda x: word2index[x], raw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66ed839b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32,  2, 57, ..., 20,  4, 41])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff8a4768",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = Embedding(vocab_size=len(vocab), dim=512)\n",
    "model = RNNCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6a3da17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "N_BATCHES = int(indices.shape[0] / BATCH_SIZE)\n",
    "BPTT = 16\n",
    "N_BPTT = int((N_BATCHES - 1) / BPTT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e70dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_indices = indices[:N_BATCHES * BATCH_SIZE]\n",
    "batched_indices = trimmed_indices.reshape(BATCH_SIZE, N_BATCHES)\n",
    "batched_indices = batched_indices.transpose()\n",
    "\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "input_batches = input_batched_indices[0:N_BPTT * BPTT]\n",
    "input_batches = input_batches.reshape(N_BPTT, BPTT, BATCH_SIZE)\n",
    "target_batches = target_batched_indices[0:N_BPTT * BPTT]\n",
    "target_batches = target_batches.reshape(N_BPTT, BPTT, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ed46d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3123, 32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_batched_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83c4811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = ''\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 10 # temperature for sampling, higher=greedier\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist = temp_dist / temp_dist.sum()\n",
    "        #m = (temp_dist > np.random.rand()).argmax() # sample from pred\n",
    "        m = output.data.argmax() # take max of predictio\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s\n",
    "\n",
    "def train(iterations=100):\n",
    "    for iter in range(iterations):\n",
    "        total_loss = 0\n",
    "        n_loss = 0\n",
    "        hidden = model.init_hidden(batch_size=BATCH_SIZE)\n",
    "        for batch_i in range(len(input_batches)):\n",
    "            hidden = Tensor(hidden.data, autograd=True)\n",
    "            loss = None\n",
    "            losses = list()\n",
    "            for t in range(BPTT):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                losses.append(batch_loss)\n",
    "                if t == 0:\n",
    "                    loss = batch_loss\n",
    "                else:\n",
    "                    loss = loss + batch_loss\n",
    "\n",
    "            for loss in losses:\n",
    "                \"\"\n",
    "                    \n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.data\n",
    "            log = \"\\r Iter:\" + str(iter)\n",
    "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
    "            log += \" - Loss:\" + str(np.exp(total_loss / (batch_i+1)))\n",
    "            if batch_i == 0:\n",
    "                log += \" - \" + generate_sample(70,'\\n').replace(\"\\n\",\" \")\n",
    "            if batch_i % 10 == 0 or batch_i-1 == len(input_batches):\n",
    "                sys.stdout.write(log)\n",
    "        optim.alpha *= 0.99\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe070651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-270f8bdb285f>:207: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return e_x / e_x.sum(axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:0 - Batch 191/195 - Loss:77.654508708888079hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      " Iter:1 - Batch 191/195 - Loss:19.899136063229825 m  se ththt thethththththththththththththththththththththththththththt\n",
      " Iter:2 - Batch 191/195 - Loss:15.189637790997436 m  an the the the the the the the the the the the the the the the the \n",
      " Iter:3 - Batch 191/195 - Loss:13.126814790770563il th the the the the the the the the the the the the the the the the \n",
      " Iter:4 - Batch 191/195 - Loss:11.974559459653904dh the the the the the the the the the the the the the the the the the\n",
      " Iter:5 - Batch 191/195 - Loss:11.159960257881073h the the the the the the the the the the the the the the the the the\n",
      " Iter:6 - Batch 191/195 - Loss:10.539842860435703h the the the the the the the the the the the the the the the the the\n",
      " Iter:7 - Batch 191/195 - Loss:10.049136635870463dh the the the the the the the the the the the the the the the the the\n",
      " Iter:8 - Batch 191/195 - Loss:9.633906828542726 th the the the the the the the the the the the the the the the the the\n",
      " Iter:9 - Batch 191/195 - Loss:9.257820325148803 th the the the the the the the the the the the the the the the the the\n",
      " Iter:10 - Batch 191/195 - Loss:8.911676840381539 ther the the the the the the the the the the the the the the the the t\n",
      " Iter:11 - Batch 191/195 - Loss:8.595994499358122 ther the the the the the the the the the the the the the the the the t\n",
      " Iter:12 - Batch 191/195 - Loss:8.305566909613013ther the the the the the the the the the the the the the the the the t\n",
      " Iter:13 - Batch 191/195 - Loss:8.039000874033768 ther the the the the the the the the the the the the the the the the t\n",
      " Iter:14 - Batch 191/195 - Loss:7.7919083841328255 ther the the the the the the the the the the the the the the the the t\n",
      " Iter:15 - Batch 191/195 - Loss:7.5607316451807425 ther the the the the the the the the the the the the the the the the t\n",
      " Iter:16 - Batch 191/195 - Loss:7.3411209642605155ther the the the the the the the the the the the the the the the the t\n",
      " Iter:17 - Batch 191/195 - Loss:7.1300116636995154fome the the the the the the the the the the the the the the the the t\n",
      " Iter:18 - Batch 191/195 - Loss:6.9265520955552835fome the the the the the the the the the the the the the the the the t\n",
      " Iter:19 - Batch 191/195 - Loss:6.7293364618176754fome the the the the the the the the the the the the the the the the t\n",
      " Iter:20 - Batch 191/195 - Loss:6.5381183499214725fome the the the the the the the the the the the the the the the the t\n",
      " Iter:21 - Batch 191/195 - Loss:6.3527043683637336fomm the the the the the the the the the the the the the the the the t\n",
      " Iter:22 - Batch 191/195 - Loss:6.1715649041103455fomm the the the the the the the the the the the the the the the the t\n",
      " Iter:23 - Batch 191/195 - Loss:5.994765198044679 fomm the the the the the the the the the the the the the the the the t\n",
      " Iter:24 - Batch 191/195 - Loss:5.8225550147112025fommand the the the the the the the the the the the the the the the th\n",
      " Iter:25 - Batch 191/195 - Loss:5.654485602506946 fommand the the the the the the the the the the the the the the the th\n",
      " Iter:26 - Batch 191/195 - Loss:5.4904001001042235fommand the the the the the the the the the the the the the the the th\n",
      " Iter:27 - Batch 191/195 - Loss:5.3300500562206845fommand the the the the the the the the the the the the the the the th\n",
      " Iter:28 - Batch 191/195 - Loss:5.1730191666263165fommand the the the the the the the the the the the the the the the th\n",
      " Iter:29 - Batch 191/195 - Loss:5.0200320181713565fear the the the the the the the the the the the the the the the the t\n",
      " Iter:30 - Batch 191/195 - Loss:4.8726514086022585fear the the the the the the the the the the the the the the the the t\n",
      " Iter:31 - Batch 191/195 - Loss:4.7305674009559056fearther the the the the the the the the the the the the the the the t\n",
      " Iter:32 - Batch 191/195 - Loss:4.5923991289627185fearther the the the the the the the the the the the the the the the t\n",
      " Iter:33 - Batch 191/195 - Loss:4.456873005672663- fearther the would the the the the the the the the the the the the the\n",
      " Iter:34 - Batch 191/195 - Loss:4.324140337275119 fearther, the the the the the the the the the the the the the the the \n",
      " Iter:35 - Batch 191/195 - Loss:4.1947165619351535fearther, the the sirstomand the the sirstomand the the sirstomand the\n",
      " Iter:36 - Batch 191/195 - Loss:4.0681383722591925 fearther, the the sirits, the the sirits, the the sirits, the the siri\n",
      " Iter:37 - Batch 191/195 - Loss:3.9430330930636774bear would the would the would the would the would the would the would\n",
      " Iter:38 - Batch 191/195 - Loss:3.8182686546553537bear would the would the would the would the would the would the would\n",
      " Iter:39 - Batch 191/195 - Loss:3.6949698196800504bay, the won the samy the won the say wastaint the would the would the\n",
      " Iter:40 - Batch 191/195 - Loss:3.5742057612977877bay, the won the samy the would the would the would the would the woul\n",
      " Iter:41 - Batch 191/195 - Loss:3.4554652661431886 bay, the won the samy the sirits, the won the samy the sirits, the won\n",
      " Iter:42 - Batch 191/195 - Loss:3.3378486327056195 bay, the won the samy the sirits, the won the samy the sirits, the won\n",
      " Iter:43 - Batch 191/195 - Loss:3.2199416648777514bay, the won the samy the sirits, the won the samy the sirits, the won\n",
      " Iter:44 - Batch 191/195 - Loss:3.1062267152766734ay, the won to the sampome the won the samy to the sird the won the s\n",
      " Iter:45 - Batch 191/195 - Loss:2.9971202906759195 bay would him the won to the speakest the would him the won to the spe\n",
      " Iter:46 - Batch 191/195 - Loss:2.8926927988250815 bay would him the won to the speakest the would him the won to the spe\n",
      " Iter:47 - Batch 191/195 - Loss:2.7921681065144393bay would him the won to the speak. Betwerd the won to the speak. Betw\n",
      " Iter:48 - Batch 191/195 - Loss:2.6956573965220008 bay would him the won to the speak. Betwerd the won to the speak. Betw\n",
      " Iter:49 - Batch 191/195 - Loss:2.6001492744165833 bay would him the won to the speak. Betwerd the won to the speak. Betw\n",
      " Iter:50 - Batch 191/195 - Loss:2.5028028498293035bay were the wastainted to the stairst Lay, the speak. Betwerd him the\n",
      " Iter:51 - Batch 191/195 - Loss:2.4101605956917385 bay were the wastainted to the wastainted to the wastainted to the was\n",
      " Iter:52 - Batch 191/195 - Loss:2.3201659651820163 bay were the wastainted to the mant woto to the may, the may, the may,\n",
      " Iter:53 - Batch 191/195 - Loss:2.2355300450862283 bay were the wastainted to the may, the may, the may, the may, the may\n",
      " Iter:54 - Batch 191/195 - Loss:2.1557379111054325bay were the wastainted to the mant woto the may, the may, the may, th\n",
      " Iter:55 - Batch 191/195 - Loss:2.0778273251787473bay were the wastainted to the wastainted to the wastainted to the was\n",
      " Iter:56 - Batch 191/195 - Loss:2.0041977161832886bay were the wastainted to the wastainted to the wastainted to the was\n",
      " Iter:57 - Batch 191/195 - Loss:1.9375180017297224 bay were the wastainted browhir hath so the may, to the mant woto be s\n",
      " Iter:58 - Batch 191/195 - Loss:1.8683494963542606ay were the wastainted browhir hath so the may, to the mant woto be s\n",
      " Iter:59 - Batch 191/195 - Loss:1.8052820463888268 bay were the wastainted bisame, and samy to the wastainted bisame, and\n",
      " Iter:60 - Batch 191/195 - Loss:1.7773325997647196bay were the wastainted bisame, and samy the mayt, and the wastainted \n",
      " Iter:61 - Batch 191/195 - Loss:1.7227520810906363 boy, and samdst Lay, sind brow not and samy to the wastainted my samn \n",
      " Iter:62 - Batch 191/195 - Loss:1.6805204214733047boy, sirithingst are samn the may, to the mantaint boy wors are no was\n",
      " Iter:63 - Batch 191/195 - Loss:1.6209785375207939 boy, here the tampont to the waster brace brow no was not to the was n\n",
      " Iter:64 - Batch 191/195 - Loss:1.5605518094427943 boy, here the mound browhow my eakest horse beggerse too, to the stair\n",
      " Iter:65 - Batch 191/195 - Loss:1.5148891605116066 boy, here in the hith a sparsomet too, this toother him, this stodembe\n",
      " Iter:66 - Batch 191/195 - Loss:1.4657605047911904 boy, sind browh, here the waster my ent wot to the wind them feak; His\n",
      " Iter:67 - Batch 191/195 - Loss:1.4310734801227252 both head.  CAES: There in the hith againg of the mantaint boy, to the\n",
      " Iter:68 - Batch 191/195 - Loss:1.3964722890302857 boy, sind brow not and somet toother begrost thou and somet toother be\n",
      " Iter:69 - Batch 191/195 - Loss:1.3812782269572266boy, sirut here the mound in them feak.  CHES: There in the nady, and \n",
      " Iter:70 - Batch 191/195 - Loss:1.4253383323971625 I he weimst are no feak.  CHES: There in the name in them feak.  CHES:\n",
      " Iter:71 - Batch 191/195 - Loss:1.3847045179249193 bay, Cillainsa prof this was not and somet too, sirdamn this grom thy \n",
      " Iter:72 - Batch 191/195 - Loss:1.3968699378708507 bother so foo be grom me to-shall sty pHESTITIM: Here man's parso feak\n",
      " Iter:73 - Batch 191/195 - Loss:1.3544103003050865 both head.  CHES: Why, Bethink to pard the sirdere of the wing Thou an\n",
      " Iter:74 - Batch 191/195 - Loss:1.2936406087536165both he weightichow, and samy is brom to your juch here in the nady, a\n",
      " Iter:75 - Batch 191/195 - Loss:1.2473448399867126 both head.  CAESTOR: And samd, to the pring of thy hair bish this stod\n",
      " Iter:76 - Batch 191/195 - Loss:1.2196907430439746 I abon was not will told.  CAELO: TITIM Th, I will too, in the mand th\n",
      " Iter:77 - Batch 191/195 - Loss:1.1945262278113673 I he weightichow, a peat shath so the steich the hath ever air, and em\n",
      " Iter:78 - Batch 191/195 - Loss:1.1774950251964875 I he weightichow, a peat shath eat, sir, man's weir: Hoth he weir all \n",
      " Iter:79 - Batch 191/195 - Loss:1.1618951229921144I abon was not and samd, to the sing fard be grus will bise thou and s\n",
      " Iter:80 - Batch 191/195 - Loss:1.1533716018564147 I he parsomet too, this stodeme the sirithing of thy hath ever all thi\n",
      " Iter:81 - Batch 191/195 - Loss:1.1433775620357027 I he parso most thou and sird way is monow not estirut I will too, and\n",
      " Iter:82 - Batch 191/195 - Loss:1.1374267338535022 I abon was not and samd, to the niat I am didingme taminat' es, and sa\n",
      " Iter:83 - Batch 191/195 - Loss:1.1242328105412396 I abon was not and she'd me to the allaing of thy hath et'd me to the \n",
      " Iter:84 - Batch 191/195 - Loss:1.1122951073482628 I abon was not and most thou and sirut hereit toother bond sirut herei\n",
      " Iter:85 - Batch 191/195 - Loss:1.1052027294319453 I abon was not and she'ch the common Mand emy is them feake she sing f\n",
      " Iter:86 - Batch 191/195 - Loss:1.0957389645711564 I abon was not and somet to the shat hesely dish thee it so untair bis\n",
      " Iter:87 - Batch 191/195 - Loss:1.0881612719782257I abon was not and somet to the shat hesely dish thee to the allaing o\n",
      " Iter:88 - Batch 191/195 - Loss:1.0814402196155581 I abon was not and somet to the shat Reak: He so and sirut hereit for \n",
      " Iter:89 - Batch 191/195 - Loss:1.0761635404704983 I abon was not and somet to the shat Reak: He so and so and sirut here\n",
      " Iter:90 - Batch 191/195 - Loss:1.0722060561778823 I abon was not and somet to the shat Reak: He so and so and sirut here\n",
      " Iter:91 - Batch 191/195 - Loss:1.0677068333854417 I abon was can make not? YOR samon, will the mounders sty and emaniarr\n",
      " Iter:92 - Batch 191/195 - Loss:1.0633761557630215 Buem feakes, ay I speak: He seemiearn Thair pard the speak: He seemiea\n",
      " Iter:93 - Batch 191/195 - Loss:1.0597562655073482 Buem farse have thee, and samonough the mounders seeming of he and som\n",
      " Iter:94 - Batch 191/195 - Loss:1.0565292974938396 Buem farse have than the may, speak.  CAES: Th, I and the offace; For.\n",
      " Iter:95 - Batch 191/195 - Loss:1.0533029094699544 Buem farse have of the moundere the flace.  CAES: Th, I and the offace\n",
      " Iter:96 - Batch 191/195 - Loss:1.0508712383336223 Buem farse have of the flace.  CAES: Th, I and the offace; For.  Secon\n",
      " Iter:97 - Batch 191/195 - Loss:1.0483371390389392 Buem farse have of the flace.  CAES: Th, I and the offace; For.  Secon\n",
      " Iter:98 - Batch 191/195 - Loss:1.0462056878817538 Buem farse have of the flace.  CAES: Th, I and the offace; For.  Secon\n",
      " Iter:99 - Batch 191/195 - Loss:1.0442456229956314 Buem farse have of the flace.  CAES: Th, I and the offace; For.  Secon\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "457df4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bure too, in thee\n",
      "Tur\n",
      "Tarr agains; and I will line thou and so your juchave abon was gabe in?\n",
      "\n",
      "LEY RATho I have my lord, and samose his head\n",
      "Mear are: out the moundere of the moundere the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "Th, I and I conflich\n",
      "the nad speak:\n",
      "He most the flace.\n",
      "\n",
      "CAES:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(n=2000, init_char='\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34d20c7",
   "metadata": {},
   "source": [
    "# SECTION 2: gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "20b13586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid Activations\n",
      "[0.93940638 0.96852968]\n",
      "[0.9919462  0.99121735]\n",
      "[0.99301385 0.99302901]\n",
      "[0.9930713  0.99307098]\n",
      "[0.99307285 0.99307285]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "[0.99307291 0.99307291]\n",
      "\n",
      "Sigmoid Gradients\n",
      "[0.03439552 0.03439552]\n",
      "[0.00118305 0.00118305]\n",
      "[4.06916726e-05 4.06916726e-05]\n",
      "[1.39961115e-06 1.39961115e-06]\n",
      "[4.81403643e-08 4.81403637e-08]\n",
      "[1.65582672e-09 1.65582765e-09]\n",
      "[5.69682675e-11 5.69667160e-11]\n",
      "[1.97259346e-12 1.97517920e-12]\n",
      "[8.45387597e-14 8.02306381e-14]\n",
      "[1.45938177e-14 2.16938983e-14]\n",
      "Activations\n",
      "[4.8135251  4.72615519]\n",
      "[23.71814585 23.98025559]\n",
      "[119.63916823 118.852839  ]\n",
      "[595.05052421 597.40951192]\n",
      "[2984.68857188 2977.61160877]\n",
      "[14895.13500696 14916.36589628]\n",
      "[74560.59859209 74496.90592414]\n",
      "[372548.22228863 372739.30029248]\n",
      "[1863505.42345854 1862932.18944699]\n",
      "[9315234.18124649 9316953.88328115]\n",
      "\n",
      "Gradients\n",
      "[5. 5.]\n",
      "[25. 25.]\n",
      "[125. 125.]\n",
      "[625. 625.]\n",
      "[3125. 3125.]\n",
      "[15625. 15625.]\n",
      "[78125. 78125.]\n",
      "[390625. 390625.]\n",
      "[1953125. 1953125.]\n",
      "[9765625. 9765625.]\n"
     ]
    }
   ],
   "source": [
    "(sigmoid,relu)=(lambda x:1/(1+np.exp(-x)), lambda x:(x>0).astype(float)*x)\n",
    "weights = np.array([[1,4],[4,1]])\n",
    "activation = sigmoid(np.array([1,0.01]))\n",
    "print(\"Sigmoid Activations\")\n",
    "activations = list()\n",
    "for iter in range(10):\n",
    "    activation = sigmoid(activation.dot(weights))\n",
    "    activations.append(activation)\n",
    "    print(activation)\n",
    "\n",
    "print(\"\\nSigmoid Gradients\")\n",
    "gradient = np.ones_like(activation)\n",
    "for activation in reversed(activations):\n",
    "    gradient = (activation * (1 - activation) * gradient)\n",
    "    gradient = gradient.dot(weights.transpose())\n",
    "    print(gradient)\n",
    "\n",
    "print(\"Activations\")\n",
    "activations = list()\n",
    "for iter in range(10):\n",
    "    activation = relu(activation.dot(weights))\n",
    "    activations.append(activation)\n",
    "    print(activation)\n",
    "\n",
    "print(\"\\nGradients\")\n",
    "gradient = np.ones_like(activation)\n",
    "for activation in reversed(activations):\n",
    "    gradient = ((activation > 0) * gradient).dot(weights.transpose())\n",
    "    print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304c8699",
   "metadata": {},
   "source": [
    "# SECTION 3: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c59eb434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor (object):\n",
    "    def __init__(self,data, autograd=False, creators=None, creation_op=None, id=None):\n",
    "        self.data = np.array(data)\n",
    "        self.creators = creators\n",
    "        self.creation_op = creation_op\n",
    "        self.grad = None\n",
    "        self.autograd = autograd\n",
    "        self.children = {}\n",
    "        self.is_recurrent = False\n",
    "        if(id is None):\n",
    "            id = np.random.randint(0,100000)\n",
    "        self.id = id\n",
    "        if(creators is not None):\n",
    "            for c in creators:\n",
    "                if(self.id not in c.children):\n",
    "                    c.children[self.id] = 1\n",
    "                else:\n",
    "                    c.children[self.id] += 1\n",
    "   \n",
    "    def all_children_grads_accounted_for(self):\n",
    "        for id,cnt in self.children.items():\n",
    "            if(cnt != 0):\n",
    "                return False\n",
    "        return True\n",
    "   \n",
    "    def backward(self,grad=None, grad_origin=None):\n",
    "        if(self.autograd):\n",
    "            \n",
    "            if(grad is None):\n",
    "                grad = Tensor(np.ones_like(self.data))\n",
    "            \n",
    "            if(grad_origin is not None):\n",
    "                if (self.children[grad_origin.id] == 0):\n",
    "                    return\n",
    "                else:\n",
    "                    self.children[grad_origin.id] -= 1\n",
    "            if(self.grad is None):\n",
    "                self.grad = grad\n",
    "            else:\n",
    "                self.grad += grad\n",
    "                \n",
    "            if(self.creators is not None and (self.all_children_grads_accounted_for() or grad_origin is None)):\n",
    "                if(self.creation_op == \"add\"):\n",
    "                    self.creators[0].backward(self.grad, self)\n",
    "                    self.creators[1].backward(self.grad, self)\n",
    "                if(self.creation_op == \"neg\"):\n",
    "                    self.creators[0].backward(self.grad.__neg__())\n",
    "                if(self.creation_op == \"sub\"):\n",
    "                    new = Tensor(self.grad.data)\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = Tensor(self.grad.__neg__().data)\n",
    "                    self.creators[1].backward(new, self)\n",
    "                if(self.creation_op == \"mul\"):\n",
    "                    new = self.grad * self.creators[1]\n",
    "                    self.creators[0].backward(new, self)\n",
    "                    new = self.grad * self.creators[0]\n",
    "                    self.creators[1].backward(new, self)\n",
    "                if(self.creation_op == \"mm\"):\n",
    "                    act = self.creators[0] # usually an activation\n",
    "                    weights = self.creators[1] # usually a weight matrix\n",
    "                    new = self.grad.mm(weights.transpose())\n",
    "                    act.backward(new)\n",
    "                    new = self.grad.transpose().mm(act).transpose()\n",
    "                    weights.backward(new)\n",
    "                if(self.creation_op == \"transpose\"):\n",
    "                    self.creators[0].backward(self.grad.transpose())\n",
    "                if(\"sum\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    ds = self.creators[0].data.shape[dim]\n",
    "                    self.creators[0].backward(self.grad.expand(dim,ds))\n",
    "                if(\"expand\" in self.creation_op):\n",
    "                    dim = int(self.creation_op.split(\"_\")[1])\n",
    "                    self.creators[0].backward(self.grad.sum(dim))\n",
    "                if(self.creation_op == \"sigmoid\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (self * (ones - self)))\n",
    "                if(self.creation_op == \"tanh\"):\n",
    "                    ones = Tensor(np.ones_like(self.grad.data))\n",
    "                    self.creators[0].backward(self.grad * (ones - (self * self)))\n",
    "                if(self.creation_op == \"index_select\"):\n",
    "                    new_grad = np.zeros_like(self.creators[0].data)\n",
    "                    indices_ = self.index_select_indices.data.flatten()\n",
    "                    grad_ = grad.data.reshape(len(indices_), -1)\n",
    "                    for i in range(len(indices_)):\n",
    "                        new_grad[indices_[i]] += grad_[i]\n",
    "                    self.creators[0].backward(Tensor(new_grad))\n",
    "                if(self.creation_op == \"cross_entropy\"):\n",
    "                    dx = self.softmax_output - self.target_dist\n",
    "                    self.creators[0].backward(Tensor(dx))\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data + other.data, autograd=True, creators=[self,other], creation_op=\"add\")\n",
    "        return Tensor(self.data + other.data)\n",
    "   \n",
    "    def __neg__(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data * -1, autograd=True, creators=[self], creation_op=\"neg\")\n",
    "        return Tensor(self.data * -1)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if(self.autograd and other.autograd): \n",
    "            return Tensor(self.data - other.data, autograd=True, creators=[self,other], creation_op=\"sub\")\n",
    "        return Tensor(self.data - other.data)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data * other.data, autograd=True, creators=[self,other], creation_op=\"mul\")\n",
    "        return Tensor(self.data * other.data)\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        if(self.autograd and other.autograd):\n",
    "            return Tensor(self.data / other.data, autograd=True, creators=[self,other], creation_op=\"div\")\n",
    "        return Tensor(self.data / other.data)\n",
    "\n",
    "    def sum(self, dim):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.sum(dim), autograd=True, creators=[self], creation_op=\"sum_\"+str(dim))\n",
    "        return Tensor(self.data.sum(dim))\n",
    "\n",
    "    def expand(self, dim,copies):\n",
    "        trans_cmd = list(range(0,len(self.data.shape)))\n",
    "        trans_cmd.insert(dim,len(self.data.shape))\n",
    "        new_shape = list(self.data.shape) + [copies]\n",
    "        new_data = self.data.repeat(copies).reshape(new_shape)\n",
    "        new_data = new_data.transpose(trans_cmd)\n",
    "        if(self.autograd):\n",
    "            return Tensor(new_data, autograd=True, creators=[self], creation_op=\"expand_\"+str(dim))\n",
    "        return Tensor(new_data)\n",
    "    \n",
    "    def transpose(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.transpose(), autograd=True, creators=[self], creation_op=\"transpose\")\n",
    "        return Tensor(self.data.transpose())\n",
    "\n",
    "    def mm(self, x):\n",
    "        if(self.autograd):\n",
    "            return Tensor(self.data.dot(x.data), autograd=True, creators=[self,x], creation_op=\"mm\")\n",
    "        return Tensor(self.data.dot(x.data))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(1 / (1 + np.exp(-self.data)),\n",
    "            autograd=True,\n",
    "            creators=[self],\n",
    "            creation_op=\"sigmoid\")\n",
    "        return Tensor(1 / (1 + np.exp(-self.data)))\n",
    "\n",
    "    def tanh(self):\n",
    "        if(self.autograd):\n",
    "            return Tensor(np.tanh(self.data),\n",
    "            autograd=True,\n",
    "            creators=[self],\n",
    "            creation_op=\"tanh\")\n",
    "        return Tensor(np.tanh(self.data))\n",
    "    \n",
    "    \n",
    "    def softmax(self):\n",
    "        e_x = np.exp(self.data - np.max(self.data))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "    \n",
    "    def index_select(self, indices):\n",
    "        if(self.autograd):\n",
    "            new = Tensor(self.data[indices.data],\n",
    "            autograd=True,\n",
    "            creators=[self],\n",
    "            creation_op=\"index_select\")\n",
    "            new.index_select_indices = indices\n",
    "            return new\n",
    "        return Tensor(self.data[indices.data])\n",
    "    \n",
    "    def cross_entropy(self, target_indices):\n",
    "        temp = np.exp(self.data)\n",
    "        softmax_output = temp / np.sum(temp,\n",
    "        axis=len(self.data.shape)-1,\n",
    "        keepdims=True)\n",
    "        t = target_indices.data.flatten()\n",
    "        p = softmax_output.reshape(len(t),-1)\n",
    "    \n",
    "        target_dist = np.eye(p.shape[1])[t]\n",
    "        loss = -(np.log(p) * (target_dist)).sum(1).mean()\n",
    "        if(self.autograd):\n",
    "            out = Tensor(loss,\n",
    "            autograd=True,\n",
    "            creators=[self],\n",
    "            creation_op=\"cross_entropy\")\n",
    "            out.softmax_output = softmax_output\n",
    "            out.target_dist = target_dist\n",
    "            return out\n",
    "        return Tensor(loss)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return str(self.data.__repr__())\n",
    "    def __str__(self):\n",
    "        return str(self.data.__str__())\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        self.parameters = list()\n",
    "    def get_parameters(self):\n",
    "        return self.parameters\n",
    "\n",
    "class Linear(Layer):\n",
    "    def __init__(self, n_inputs, n_outputs, bias=True):\n",
    "        super().__init__()\n",
    "        W = np.random.randn(n_inputs, n_outputs)*np.sqrt(2.0/(n_inputs))\n",
    "        self.weight = Tensor(W, autograd=True)\n",
    "        self.parameters.append(self.weight)\n",
    "        if bias:\n",
    "            self.bias = Tensor(np.zeros(n_outputs), autograd=True)\n",
    "            self.parameters.append(self.bias)\n",
    "        else:\n",
    "            self.bias = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        temp = input.mm(self.weight)\n",
    "        if self.bias:\n",
    "            temp += self.bias.expand(0,len(input.data))\n",
    "        return temp\n",
    "    \n",
    "class Embedding(Layer):\n",
    "    def __init__(self, vocab_size, dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dim = dim\n",
    "        # this initialiation style is just a convention from word2vec\n",
    "        weight = (np.random.rand(vocab_size, dim) - 0.5) / dim\n",
    "        self.weight = Tensor(weight, autograd=True)\n",
    "        self.parameters.append(self.weight)\n",
    "    def forward(self, input):\n",
    "         return self.weight.index_select(input)\n",
    "    \n",
    "class CrossEntropyLoss(object):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, input, target):\n",
    "        return input.cross_entropy(target)\n",
    "\n",
    "class RNNCell(Layer):\n",
    "    def __init__(self, n_inputs,n_hidden,n_output,activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        if(activation == 'sigmoid'):\n",
    "            self.activation = Sigmoid()\n",
    "        elif(activation == 'tanh'):\n",
    "            self.activation == Tanh()\n",
    "        else:\n",
    "            raise Exception(\"Non-linearity not found\")\n",
    "        self.w_ih = Linear(n_inputs, n_hidden)\n",
    "        self.w_hh = Linear(n_hidden, n_hidden)\n",
    "        self.w_ho = Linear(n_hidden, n_output)\n",
    "        self.parameters += self.w_ih.get_parameters()\n",
    "        self.parameters += self.w_hh.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        from_prev_hidden = self.w_hh.forward(hidden)\n",
    "        combined = self.w_ih.forward(input) + from_prev_hidden\n",
    "        new_hidden = self.activation.forward(combined)\n",
    "        output = self.w_ho.forward(new_hidden)\n",
    "        return output, new_hidden\n",
    "    def init_hidden(self, batch_size=1):\n",
    "        return Tensor(np.zeros((batch_size,self.n_hidden)),autograd=True)\n",
    "\n",
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, input):\n",
    "        return input.sigmoid()\n",
    "\n",
    "class SGD(object):\n",
    "    def __init__(self, parameters, alpha=0.1):\n",
    "        self.parameters = parameters\n",
    "        self.alpha = alpha\n",
    "    def zero(self):\n",
    "        for p in self.parameters:\n",
    "            p.grad.data *= 0\n",
    "    def step(self, zero=True):\n",
    "        for p in self.parameters:\n",
    "            p.data -= p.grad.data * self.alpha\n",
    "            if(zero):\n",
    "                p.grad.data *= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bd79e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(Layer):\n",
    "    \n",
    "    \n",
    "    def __init__(self, n_inputs, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.xf = Linear(n_inputs, n_hidden)\n",
    "        self.xi = Linear(n_inputs, n_hidden)\n",
    "        self.xo = Linear(n_inputs, n_hidden)\n",
    "        self.xc = Linear(n_inputs, n_hidden)\n",
    "        self.hf = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hi = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.ho = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.hc = Linear(n_hidden, n_hidden, bias=False)\n",
    "        self.w_ho = Linear(n_hidden, n_output, bias=False)\n",
    "        self.parameters += self.xf.get_parameters()\n",
    "        self.parameters += self.xi.get_parameters()\n",
    "        self.parameters += self.xo.get_parameters()\n",
    "        self.parameters += self.xc.get_parameters()\n",
    "        self.parameters += self.hf.get_parameters()\n",
    "        self.parameters += self.hi.get_parameters()\n",
    "        self.parameters += self.ho.get_parameters()\n",
    "        self.parameters += self.hc.get_parameters()\n",
    "        self.parameters += self.w_ho.get_parameters()\n",
    "    \n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        prev_hidden = hidden[0]\n",
    "        prev_cell = hidden[1]\n",
    "        f = (self.xf.forward(input)+self.hf.forward(prev_hidden)).sigmoid()\n",
    "        i = (self.xi.forward(input)+self.hi.forward(prev_hidden)).sigmoid()\n",
    "        o = (self.xo.forward(input)+self.ho.forward(prev_hidden)).sigmoid()\n",
    "        g = (self.xc.forward(input) +self.hc.forward(prev_hidden)).tanh()\n",
    "        c = (f * prev_cell) + (i * g)\n",
    "        h = o * c.tanh()\n",
    "        h.is_recurrent = True\n",
    "        output = self.w_ho.forward(h)\n",
    "        return output, (h, c)\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        h = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "        c = Tensor(np.zeros((batch_size, self.n_hidden)), autograd=True)\n",
    "        h.data[:,0] += 1\n",
    "        c.data[:,0] += 1\n",
    "        return (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0becdedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "N_BATCHES = int(indices.shape[0] / BATCH_SIZE)\n",
    "BPTT = 25\n",
    "N_BPTT = int((N_BATCHES - 1) / BPTT)\n",
    "\n",
    "\n",
    "embed = Embedding(vocab_size=len(vocab), dim=512)\n",
    "model = LSTMCell(n_inputs=512, n_hidden=512, n_output=len(vocab))\n",
    "model.w_ho.weight.data *= 0 # this seemed to help training\n",
    "criterion = CrossEntropyLoss()\n",
    "optim = SGD(parameters=model.get_parameters() + embed.get_parameters(), alpha=0.05)\n",
    "\n",
    "trimmed_indices = indices[0:N_BATCHES*BATCH_SIZE]\n",
    "batched_indices = trimmed_indices.reshape(BATCH_SIZE, N_BATCHES)\n",
    "batched_indices = batched_indices.transpose()\n",
    "input_batched_indices = batched_indices[0:-1]\n",
    "target_batched_indices = batched_indices[1:]\n",
    "\n",
    "input_batches = input_batched_indices[:N_BPTT*BPTT]\n",
    "input_batches = input_batches.reshape(N_BPTT, BPTT, BATCH_SIZE)\n",
    "target_batches = target_batched_indices[:N_BPTT*BPTT]\n",
    "target_batches = target_batches.reshape(N_BPTT, BPTT, BATCH_SIZE)\n",
    "min_loss = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3592f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(n=30, init_char=' '):\n",
    "    s = \"\"\n",
    "    hidden = model.init_hidden(batch_size=1)\n",
    "    input = Tensor(np.array([word2index[init_char]]))\n",
    "    for i in range(n):\n",
    "        rnn_input = embed.forward(input)\n",
    "        output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "        output.data *= 15\n",
    "        temp_dist = output.softmax()\n",
    "        temp_dist /= temp_dist.sum()\n",
    "        # m = (temp_dist > np.random.rand()).argmax() # sample from pred\n",
    "        m = output.data.argmax() # take the max prediction\n",
    "        c = vocab[m]\n",
    "        input = Tensor(np.array([m]))\n",
    "        s += c\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b7cf5865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(iterations=100, min_loss=1000):\n",
    "    for iter in range(iterations):\n",
    "        total_loss, n_loss = (0, 0)\n",
    "        hidden = model.init_hidden(batch_size=BATCH_SIZE)\n",
    "        batches_to_train = len(input_batches)\n",
    "        for batch_i in range(batches_to_train):\n",
    "            hidden = (Tensor(hidden[0].data, autograd=True), Tensor(hidden[1].data, autograd=True))\n",
    "            losses = list()\n",
    "            for t in range(BPTT):\n",
    "                input = Tensor(input_batches[batch_i][t], autograd=True)\n",
    "                rnn_input = embed.forward(input=input)\n",
    "                output, hidden = model.forward(input=rnn_input, hidden=hidden)\n",
    "                target = Tensor(target_batches[batch_i][t], autograd=True)\n",
    "                batch_loss = criterion.forward(output, target)\n",
    "                if(t == 0):\n",
    "                    losses.append(batch_loss)\n",
    "                else:\n",
    "                    losses.append(batch_loss + losses[-1])\n",
    "            loss = losses[-1]\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.data / BPTT\n",
    "            epoch_loss = np.exp(total_loss / (batch_i+1))\n",
    "            if(epoch_loss < min_loss):\n",
    "                min_loss = epoch_loss\n",
    "                print()\n",
    "\n",
    "            log = \"\\r Iter:\" + str(iter)\n",
    "            log += \" - Alpha:\" + str(optim.alpha)[0:5]\n",
    "            log += \" - Batch \"+str(batch_i+1)+\"/\"+str(len(input_batches))\n",
    "            log += \" - Min Loss:\" + str(min_loss)[0:5]\n",
    "            log += \" - Loss:\" + str(epoch_loss)\n",
    "            if(batch_i % 50 == 0):\n",
    "                s = generate_sample(n=70, init_char='T').replace(\"\\n\",\" \")\n",
    "                log += \" - \" + s\n",
    "            sys.stdout.write(log)\n",
    "\n",
    "        optim.alpha *= 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7defe6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Iter:0 - Alpha:0.05 - Batch 3/249 - Min Loss:61.69 - Loss:61.748295982332845 -                                                                       \n",
      " Iter:0 - Alpha:0.05 - Batch 4/249 - Min Loss:61.60 - Loss:61.60517403961742\n",
      " Iter:0 - Alpha:0.05 - Batch 5/249 - Min Loss:61.29 - Loss:61.29442397158086\n",
      " Iter:0 - Alpha:0.05 - Batch 6/249 - Min Loss:60.66 - Loss:60.66382011246509\n",
      " Iter:0 - Alpha:0.05 - Batch 7/249 - Min Loss:59.58 - Loss:59.58905249944992\n",
      " Iter:0 - Alpha:0.05 - Batch 8/249 - Min Loss:57.14 - Loss:57.149968333866674\n",
      " Iter:0 - Alpha:0.05 - Batch 9/249 - Min Loss:53.31 - Loss:53.313391207757725\n",
      " Iter:0 - Alpha:0.05 - Batch 10/249 - Min Loss:51.61 - Loss:51.616747820150856\n",
      " Iter:0 - Alpha:0.05 - Batch 11/249 - Min Loss:50.18 - Loss:50.18998224999764\n",
      " Iter:0 - Alpha:0.05 - Batch 12/249 - Min Loss:48.36 - Loss:48.36974307417649\n",
      " Iter:0 - Alpha:0.05 - Batch 13/249 - Min Loss:46.02 - Loss:46.02249114726887\n",
      " Iter:0 - Alpha:0.05 - Batch 14/249 - Min Loss:45.25 - Loss:45.258228974902345\n",
      " Iter:0 - Alpha:0.05 - Batch 15/249 - Min Loss:44.00 - Loss:44.00493170085594\n",
      " Iter:0 - Alpha:0.05 - Batch 16/249 - Min Loss:42.84 - Loss:42.84933966339319\n",
      " Iter:0 - Alpha:0.05 - Batch 17/249 - Min Loss:41.77 - Loss:41.77144501972203\n",
      " Iter:0 - Alpha:0.05 - Batch 18/249 - Min Loss:41.04 - Loss:41.045167807969364\n",
      " Iter:0 - Alpha:0.05 - Batch 19/249 - Min Loss:40.49 - Loss:40.49426774147219\n",
      " Iter:0 - Alpha:0.05 - Batch 20/249 - Min Loss:39.71 - Loss:39.710979210730784\n",
      " Iter:0 - Alpha:0.05 - Batch 21/249 - Min Loss:38.80 - Loss:38.80496722599705\n",
      " Iter:0 - Alpha:0.05 - Batch 22/249 - Min Loss:38.61 - Loss:38.61285314582181\n",
      " Iter:0 - Alpha:0.05 - Batch 23/249 - Min Loss:38.48 - Loss:38.48333034248521\n",
      " Iter:0 - Alpha:0.05 - Batch 24/249 - Min Loss:37.78 - Loss:37.78528982689385\n",
      " Iter:0 - Alpha:0.05 - Batch 25/249 - Min Loss:37.24 - Loss:37.248499949163744\n",
      " Iter:0 - Alpha:0.05 - Batch 26/249 - Min Loss:37.03 - Loss:37.03107928951554\n",
      " Iter:0 - Alpha:0.05 - Batch 27/249 - Min Loss:36.66 - Loss:36.669404487389144\n",
      " Iter:0 - Alpha:0.05 - Batch 28/249 - Min Loss:36.12 - Loss:36.12672793046131\n",
      " Iter:0 - Alpha:0.05 - Batch 29/249 - Min Loss:35.72 - Loss:35.72586255068767\n",
      " Iter:0 - Alpha:0.05 - Batch 30/249 - Min Loss:35.30 - Loss:35.30353974151329\n",
      " Iter:0 - Alpha:0.05 - Batch 31/249 - Min Loss:35.03 - Loss:35.036554962219675\n",
      " Iter:0 - Alpha:0.05 - Batch 32/249 - Min Loss:34.69 - Loss:34.69995421849567\n",
      " Iter:0 - Alpha:0.05 - Batch 33/249 - Min Loss:34.38 - Loss:34.38570544239023\n",
      " Iter:0 - Alpha:0.05 - Batch 34/249 - Min Loss:34.09 - Loss:34.09969868883655\n",
      " Iter:0 - Alpha:0.05 - Batch 35/249 - Min Loss:33.93 - Loss:33.931891243104026\n",
      " Iter:0 - Alpha:0.05 - Batch 36/249 - Min Loss:33.75 - Loss:33.758670956556045\n",
      " Iter:0 - Alpha:0.05 - Batch 37/249 - Min Loss:33.48 - Loss:33.48518183441157\n",
      " Iter:0 - Alpha:0.05 - Batch 38/249 - Min Loss:33.29 - Loss:33.296329660630974\n",
      " Iter:0 - Alpha:0.05 - Batch 39/249 - Min Loss:33.28 - Loss:33.289912723608396\n",
      " Iter:0 - Alpha:0.05 - Batch 40/249 - Min Loss:33.01 - Loss:33.01278789683455\n",
      " Iter:0 - Alpha:0.05 - Batch 42/249 - Min Loss:32.96 - Loss:33.052408389520004\n",
      " Iter:0 - Alpha:0.05 - Batch 43/249 - Min Loss:32.82 - Loss:32.824889413200744\n",
      " Iter:0 - Alpha:0.05 - Batch 44/249 - Min Loss:32.62 - Loss:32.62366597619854\n",
      " Iter:0 - Alpha:0.05 - Batch 45/249 - Min Loss:32.50 - Loss:32.50490626647283\n",
      " Iter:0 - Alpha:0.05 - Batch 46/249 - Min Loss:32.32 - Loss:32.326534046221994\n",
      " Iter:0 - Alpha:0.05 - Batch 47/249 - Min Loss:32.16 - Loss:32.164168792081014\n",
      " Iter:0 - Alpha:0.05 - Batch 48/249 - Min Loss:31.97 - Loss:31.9763867628552\n",
      " Iter:0 - Alpha:0.05 - Batch 49/249 - Min Loss:31.83 - Loss:31.83430512131159\n",
      " Iter:0 - Alpha:0.05 - Batch 50/249 - Min Loss:31.60 - Loss:31.609313831960396\n",
      " Iter:0 - Alpha:0.05 - Batch 51/249 - Min Loss:31.35 - Loss:31.353615117301533 -  te teeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee\n",
      " Iter:0 - Alpha:0.05 - Batch 52/249 - Min Loss:31.18 - Loss:31.187765335701773\n",
      " Iter:0 - Alpha:0.05 - Batch 53/249 - Min Loss:31.04 - Loss:31.046487114851125\n",
      " Iter:0 - Alpha:0.05 - Batch 54/249 - Min Loss:30.95 - Loss:30.951914326504703\n",
      " Iter:0 - Alpha:0.05 - Batch 55/249 - Min Loss:30.78 - Loss:30.780029801799877\n",
      " Iter:0 - Alpha:0.05 - Batch 56/249 - Min Loss:30.62 - Loss:30.624541719259565\n",
      " Iter:0 - Alpha:0.05 - Batch 57/249 - Min Loss:30.41 - Loss:30.416906037631527\n",
      " Iter:0 - Alpha:0.05 - Batch 58/249 - Min Loss:30.26 - Loss:30.260100358426076\n",
      " Iter:0 - Alpha:0.05 - Batch 59/249 - Min Loss:30.07 - Loss:30.077475072563995\n",
      " Iter:0 - Alpha:0.05 - Batch 60/249 - Min Loss:29.91 - Loss:29.916826415608764\n",
      " Iter:0 - Alpha:0.05 - Batch 61/249 - Min Loss:29.70 - Loss:29.70648289705113\n",
      " Iter:0 - Alpha:0.05 - Batch 62/249 - Min Loss:29.53 - Loss:29.53821137506468\n",
      " Iter:0 - Alpha:0.05 - Batch 63/249 - Min Loss:29.32 - Loss:29.320940390079993\n",
      " Iter:0 - Alpha:0.05 - Batch 64/249 - Min Loss:29.13 - Loss:29.139525652939913\n",
      " Iter:0 - Alpha:0.05 - Batch 65/249 - Min Loss:29.07 - Loss:29.07118068890417\n",
      " Iter:0 - Alpha:0.05 - Batch 66/249 - Min Loss:29.01 - Loss:29.019886165126586\n",
      " Iter:0 - Alpha:0.05 - Batch 67/249 - Min Loss:28.89 - Loss:28.89416690300807\n",
      " Iter:0 - Alpha:0.05 - Batch 68/249 - Min Loss:28.69 - Loss:28.692288940928997\n",
      " Iter:0 - Alpha:0.05 - Batch 69/249 - Min Loss:28.60 - Loss:28.60340937635256\n",
      " Iter:0 - Alpha:0.05 - Batch 70/249 - Min Loss:28.46 - Loss:28.466325352686294\n",
      " Iter:0 - Alpha:0.05 - Batch 71/249 - Min Loss:28.38 - Loss:28.385655703563607\n",
      " Iter:0 - Alpha:0.05 - Batch 72/249 - Min Loss:28.36 - Loss:28.36214854135426\n",
      " Iter:0 - Alpha:0.05 - Batch 73/249 - Min Loss:28.27 - Loss:28.27851163573628\n",
      " Iter:0 - Alpha:0.05 - Batch 74/249 - Min Loss:28.11 - Loss:28.112277976141772\n",
      " Iter:0 - Alpha:0.05 - Batch 75/249 - Min Loss:27.99 - Loss:27.99511434928237\n",
      " Iter:0 - Alpha:0.05 - Batch 76/249 - Min Loss:27.90 - Loss:27.906947748230575\n",
      " Iter:0 - Alpha:0.05 - Batch 77/249 - Min Loss:27.78 - Loss:27.783088730277132\n",
      " Iter:0 - Alpha:0.05 - Batch 78/249 - Min Loss:27.71 - Loss:27.71511402373792\n",
      " Iter:0 - Alpha:0.05 - Batch 79/249 - Min Loss:27.60 - Loss:27.607902759210752\n",
      " Iter:0 - Alpha:0.05 - Batch 80/249 - Min Loss:27.50 - Loss:27.50518348142803\n",
      " Iter:0 - Alpha:0.05 - Batch 81/249 - Min Loss:27.36 - Loss:27.360788395397748\n",
      " Iter:0 - Alpha:0.05 - Batch 82/249 - Min Loss:27.28 - Loss:27.281589729527287\n",
      " Iter:0 - Alpha:0.05 - Batch 86/249 - Min Loss:27.20 - Loss:27.302731880408843\n",
      " Iter:0 - Alpha:0.05 - Batch 87/249 - Min Loss:27.14 - Loss:27.14749756858168\n",
      " Iter:0 - Alpha:0.05 - Batch 88/249 - Min Loss:27.00 - Loss:27.007610200798194\n",
      " Iter:0 - Alpha:0.05 - Batch 89/249 - Min Loss:26.86 - Loss:26.86581320919606\n",
      " Iter:0 - Alpha:0.05 - Batch 90/249 - Min Loss:26.73 - Loss:26.734617678464055\n",
      " Iter:0 - Alpha:0.05 - Batch 91/249 - Min Loss:26.60 - Loss:26.605373676208686\n",
      " Iter:0 - Alpha:0.05 - Batch 92/249 - Min Loss:26.50 - Loss:26.503497283458255\n",
      " Iter:0 - Alpha:0.05 - Batch 93/249 - Min Loss:26.35 - Loss:26.351468283504392\n",
      " Iter:0 - Alpha:0.05 - Batch 94/249 - Min Loss:26.23 - Loss:26.237445955410756\n",
      " Iter:0 - Alpha:0.05 - Batch 95/249 - Min Loss:26.11 - Loss:26.11791388169266\n",
      " Iter:0 - Alpha:0.05 - Batch 96/249 - Min Loss:26.06 - Loss:26.066188716141994\n",
      " Iter:0 - Alpha:0.05 - Batch 97/249 - Min Loss:25.96 - Loss:25.960372130028645\n",
      " Iter:0 - Alpha:0.05 - Batch 98/249 - Min Loss:25.85 - Loss:25.855632311489167\n",
      " Iter:0 - Alpha:0.05 - Batch 99/249 - Min Loss:25.71 - Loss:25.71552626695869\n",
      " Iter:0 - Alpha:0.05 - Batch 100/249 - Min Loss:25.60 - Loss:25.600667499609592\n",
      " Iter:0 - Alpha:0.05 - Batch 101/249 - Min Loss:25.50 - Loss:25.505362113488566 -  w th t m t m m m m m m m m m m m m m m m m m m m m m m m m m m m m m \n",
      " Iter:0 - Alpha:0.05 - Batch 102/249 - Min Loss:25.42 - Loss:25.425173070937024\n",
      " Iter:0 - Alpha:0.05 - Batch 103/249 - Min Loss:25.37 - Loss:25.37643071008966\n",
      " Iter:0 - Alpha:0.05 - Batch 104/249 - Min Loss:25.29 - Loss:25.29872978533467\n",
      " Iter:0 - Alpha:0.05 - Batch 105/249 - Min Loss:25.21 - Loss:25.217645983035663\n",
      " Iter:0 - Alpha:0.05 - Batch 106/249 - Min Loss:25.11 - Loss:25.11800914978132\n",
      " Iter:0 - Alpha:0.05 - Batch 107/249 - Min Loss:25.05 - Loss:25.056203828184582\n",
      " Iter:0 - Alpha:0.05 - Batch 108/249 - Min Loss:24.95 - Loss:24.95453119444191\n",
      " Iter:0 - Alpha:0.05 - Batch 109/249 - Min Loss:24.87 - Loss:24.87006996103221\n",
      " Iter:0 - Alpha:0.05 - Batch 110/249 - Min Loss:24.81 - Loss:24.81137925490959\n",
      " Iter:0 - Alpha:0.05 - Batch 111/249 - Min Loss:24.77 - Loss:24.775349627006293\n",
      " Iter:0 - Alpha:0.05 - Batch 112/249 - Min Loss:24.68 - Loss:24.688646073122083\n",
      " Iter:0 - Alpha:0.05 - Batch 113/249 - Min Loss:24.59 - Loss:24.59369284798346\n",
      " Iter:0 - Alpha:0.05 - Batch 114/249 - Min Loss:24.50 - Loss:24.506868914772586\n",
      " Iter:0 - Alpha:0.05 - Batch 115/249 - Min Loss:24.41 - Loss:24.411756564616628\n",
      " Iter:0 - Alpha:0.05 - Batch 116/249 - Min Loss:24.32 - Loss:24.323765791795196\n",
      " Iter:0 - Alpha:0.05 - Batch 117/249 - Min Loss:24.25 - Loss:24.256508330802117\n",
      " Iter:0 - Alpha:0.05 - Batch 118/249 - Min Loss:24.16 - Loss:24.169277497770313\n",
      " Iter:0 - Alpha:0.05 - Batch 119/249 - Min Loss:24.11 - Loss:24.11480790922994\n",
      " Iter:0 - Alpha:0.05 - Batch 120/249 - Min Loss:24.03 - Loss:24.03112448070028\n",
      " Iter:0 - Alpha:0.05 - Batch 121/249 - Min Loss:23.95 - Loss:23.953357582733055\n",
      " Iter:0 - Alpha:0.05 - Batch 122/249 - Min Loss:23.86 - Loss:23.867125204893792\n",
      " Iter:0 - Alpha:0.05 - Batch 123/249 - Min Loss:23.77 - Loss:23.77731615224178\n",
      " Iter:0 - Alpha:0.05 - Batch 124/249 - Min Loss:23.68 - Loss:23.68738932241471\n",
      " Iter:0 - Alpha:0.05 - Batch 125/249 - Min Loss:23.60 - Loss:23.607482413470997\n",
      " Iter:0 - Alpha:0.05 - Batch 126/249 - Min Loss:23.54 - Loss:23.542678951241783\n",
      " Iter:0 - Alpha:0.05 - Batch 127/249 - Min Loss:23.46 - Loss:23.46961429337534\n",
      " Iter:0 - Alpha:0.05 - Batch 128/249 - Min Loss:23.37 - Loss:23.371269823904875\n",
      " Iter:0 - Alpha:0.05 - Batch 129/249 - Min Loss:23.29 - Loss:23.29423044278895\n",
      " Iter:0 - Alpha:0.05 - Batch 130/249 - Min Loss:23.23 - Loss:23.230836879838265\n",
      " Iter:0 - Alpha:0.05 - Batch 131/249 - Min Loss:23.14 - Loss:23.14778209276547\n",
      " Iter:0 - Alpha:0.05 - Batch 132/249 - Min Loss:23.06 - Loss:23.061580441572033\n",
      " Iter:0 - Alpha:0.05 - Batch 133/249 - Min Loss:22.97 - Loss:22.978957070875442\n",
      " Iter:0 - Alpha:0.05 - Batch 134/249 - Min Loss:22.89 - Loss:22.89080391525283\n",
      " Iter:0 - Alpha:0.05 - Batch 135/249 - Min Loss:22.82 - Loss:22.822121187905886\n",
      " Iter:0 - Alpha:0.05 - Batch 136/249 - Min Loss:22.74 - Loss:22.748543655980704\n",
      " Iter:0 - Alpha:0.05 - Batch 137/249 - Min Loss:22.68 - Loss:22.688093615175426\n",
      " Iter:0 - Alpha:0.05 - Batch 139/249 - Min Loss:22.61 - Loss:22.643491683105687\n",
      " Iter:0 - Alpha:0.05 - Batch 141/249 - Min Loss:22.60 - Loss:22.64900567876898\n",
      " Iter:0 - Alpha:0.05 - Batch 142/249 - Min Loss:22.59 - Loss:22.59965847476648\n",
      " Iter:0 - Alpha:0.05 - Batch 143/249 - Min Loss:22.54 - Loss:22.545477095216448\n",
      " Iter:0 - Alpha:0.05 - Batch 144/249 - Min Loss:22.53 - Loss:22.532888897898616\n",
      " Iter:0 - Alpha:0.05 - Batch 145/249 - Min Loss:22.46 - Loss:22.465686960474866\n",
      " Iter:0 - Alpha:0.05 - Batch 146/249 - Min Loss:22.41 - Loss:22.4179178215095\n",
      " Iter:0 - Alpha:0.05 - Batch 147/249 - Min Loss:22.37 - Loss:22.378418561728793\n",
      " Iter:0 - Alpha:0.05 - Batch 148/249 - Min Loss:22.31 - Loss:22.314957541253793\n",
      " Iter:0 - Alpha:0.05 - Batch 149/249 - Min Loss:22.24 - Loss:22.248093057195547\n",
      " Iter:0 - Alpha:0.05 - Batch 150/249 - Min Loss:22.18 - Loss:22.18561465057278\n",
      " Iter:0 - Alpha:0.05 - Batch 151/249 - Min Loss:22.14 - Loss:22.141488618514288 - he there there there there there there there there there there there t\n",
      " Iter:0 - Alpha:0.05 - Batch 152/249 - Min Loss:22.07 - Loss:22.079997419615125\n",
      " Iter:0 - Alpha:0.05 - Batch 153/249 - Min Loss:22.00 - Loss:22.002538113763165\n",
      " Iter:0 - Alpha:0.05 - Batch 154/249 - Min Loss:21.93 - Loss:21.938078215373036\n",
      " Iter:0 - Alpha:0.05 - Batch 155/249 - Min Loss:21.90 - Loss:21.900295783067847\n",
      " Iter:0 - Alpha:0.05 - Batch 156/249 - Min Loss:21.87 - Loss:21.87389859802982\n",
      " Iter:0 - Alpha:0.05 - Batch 157/249 - Min Loss:21.82 - Loss:21.8276635832639\n",
      " Iter:0 - Alpha:0.05 - Batch 158/249 - Min Loss:21.77 - Loss:21.77414350205894\n",
      " Iter:0 - Alpha:0.05 - Batch 159/249 - Min Loss:21.73 - Loss:21.738993466583604\n",
      " Iter:0 - Alpha:0.05 - Batch 160/249 - Min Loss:21.68 - Loss:21.68456931800572\n",
      " Iter:0 - Alpha:0.05 - Batch 161/249 - Min Loss:21.62 - Loss:21.62717845959644\n",
      " Iter:0 - Alpha:0.05 - Batch 162/249 - Min Loss:21.57 - Loss:21.57562482619409\n",
      " Iter:0 - Alpha:0.05 - Batch 163/249 - Min Loss:21.52 - Loss:21.522859507769432\n",
      " Iter:0 - Alpha:0.05 - Batch 164/249 - Min Loss:21.47 - Loss:21.47857846338251\n",
      " Iter:0 - Alpha:0.05 - Batch 165/249 - Min Loss:21.43 - Loss:21.43581909907842\n",
      " Iter:0 - Alpha:0.05 - Batch 166/249 - Min Loss:21.38 - Loss:21.389163678887755\n",
      " Iter:0 - Alpha:0.05 - Batch 167/249 - Min Loss:21.36 - Loss:21.360885311068195\n",
      " Iter:0 - Alpha:0.05 - Batch 168/249 - Min Loss:21.32 - Loss:21.324747900813023\n",
      " Iter:0 - Alpha:0.05 - Batch 169/249 - Min Loss:21.27 - Loss:21.272176632260194\n",
      " Iter:0 - Alpha:0.05 - Batch 170/249 - Min Loss:21.23 - Loss:21.231655410307265\n",
      " Iter:0 - Alpha:0.05 - Batch 171/249 - Min Loss:21.19 - Loss:21.19674956669311\n",
      " Iter:0 - Alpha:0.05 - Batch 172/249 - Min Loss:21.16 - Loss:21.16656954034698\n",
      " Iter:0 - Alpha:0.05 - Batch 173/249 - Min Loss:21.12 - Loss:21.125420818462544\n",
      " Iter:0 - Alpha:0.05 - Batch 174/249 - Min Loss:21.08 - Loss:21.089454651212595\n",
      " Iter:0 - Alpha:0.05 - Batch 175/249 - Min Loss:21.05 - Loss:21.059459448231014\n",
      " Iter:0 - Alpha:0.05 - Batch 176/249 - Min Loss:21.02 - Loss:21.021190583326074\n",
      " Iter:0 - Alpha:0.05 - Batch 177/249 - Min Loss:20.97 - Loss:20.978041840991242\n",
      " Iter:0 - Alpha:0.05 - Batch 178/249 - Min Loss:20.93 - Loss:20.9329165710102\n",
      " Iter:0 - Alpha:0.05 - Batch 179/249 - Min Loss:20.91 - Loss:20.9144668312056\n",
      " Iter:0 - Alpha:0.05 - Batch 180/249 - Min Loss:20.87 - Loss:20.872626092791702\n",
      " Iter:0 - Alpha:0.05 - Batch 181/249 - Min Loss:20.82 - Loss:20.82868516967156\n",
      " Iter:0 - Alpha:0.05 - Batch 182/249 - Min Loss:20.78 - Loss:20.783705221471273\n",
      " Iter:0 - Alpha:0.05 - Batch 183/249 - Min Loss:20.73 - Loss:20.73481066375076\n",
      " Iter:0 - Alpha:0.05 - Batch 184/249 - Min Loss:20.70 - Loss:20.702142478276993\n",
      " Iter:0 - Alpha:0.05 - Batch 185/249 - Min Loss:20.66 - Loss:20.66996886527586\n",
      " Iter:0 - Alpha:0.05 - Batch 186/249 - Min Loss:20.66 - Loss:20.66241999103263\n",
      " Iter:0 - Alpha:0.05 - Batch 187/249 - Min Loss:20.62 - Loss:20.625621460418024\n",
      " Iter:0 - Alpha:0.05 - Batch 188/249 - Min Loss:20.59 - Loss:20.594578104757904\n",
      " Iter:0 - Alpha:0.05 - Batch 189/249 - Min Loss:20.56 - Loss:20.56190957944232\n",
      " Iter:0 - Alpha:0.05 - Batch 190/249 - Min Loss:20.53 - Loss:20.538499640308178\n",
      " Iter:0 - Alpha:0.05 - Batch 191/249 - Min Loss:20.50 - Loss:20.507819378993904\n",
      " Iter:0 - Alpha:0.05 - Batch 192/249 - Min Loss:20.48 - Loss:20.480158283735427\n",
      " Iter:0 - Alpha:0.05 - Batch 193/249 - Min Loss:20.41 - Loss:20.414882798199717\n",
      " Iter:0 - Alpha:0.05 - Batch 194/249 - Min Loss:20.38 - Loss:20.38993195534145\n",
      " Iter:0 - Alpha:0.05 - Batch 195/249 - Min Loss:20.35 - Loss:20.359644656482676\n",
      " Iter:0 - Alpha:0.05 - Batch 196/249 - Min Loss:20.31 - Loss:20.31251114309432\n",
      " Iter:0 - Alpha:0.05 - Batch 197/249 - Min Loss:20.28 - Loss:20.284564711176223\n",
      " Iter:0 - Alpha:0.05 - Batch 198/249 - Min Loss:20.27 - Loss:20.27497355718065\n",
      " Iter:0 - Alpha:0.05 - Batch 199/249 - Min Loss:20.23 - Loss:20.239180688433315\n",
      " Iter:0 - Alpha:0.05 - Batch 200/249 - Min Loss:20.19 - Loss:20.194620187161128\n",
      " Iter:0 - Alpha:0.05 - Batch 201/249 - Min Loss:20.16 - Loss:20.169548112635443 - herer,  Therer,  Therer,  Therer,  Therer,  Therer,  Therer,  Therer, \n",
      " Iter:0 - Alpha:0.05 - Batch 202/249 - Min Loss:20.14 - Loss:20.146656831424387\n",
      " Iter:0 - Alpha:0.05 - Batch 203/249 - Min Loss:20.11 - Loss:20.112382582235515\n",
      " Iter:0 - Alpha:0.05 - Batch 204/249 - Min Loss:20.07 - Loss:20.07911758428587\n",
      " Iter:0 - Alpha:0.05 - Batch 205/249 - Min Loss:20.05 - Loss:20.050416948338498\n",
      " Iter:0 - Alpha:0.05 - Batch 206/249 - Min Loss:20.01 - Loss:20.011387741654424\n",
      " Iter:0 - Alpha:0.05 - Batch 207/249 - Min Loss:19.97 - Loss:19.976538199057384\n",
      " Iter:0 - Alpha:0.05 - Batch 208/249 - Min Loss:19.94 - Loss:19.942393964013718\n",
      " Iter:0 - Alpha:0.05 - Batch 209/249 - Min Loss:19.89 - Loss:19.898355321086296\n",
      " Iter:0 - Alpha:0.05 - Batch 210/249 - Min Loss:19.85 - Loss:19.85623642247839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:0 - Alpha:0.05 - Batch 211/249 - Min Loss:19.80 - Loss:19.80500357737236\n",
      " Iter:0 - Alpha:0.05 - Batch 212/249 - Min Loss:19.76 - Loss:19.76522124477926\n",
      " Iter:0 - Alpha:0.05 - Batch 213/249 - Min Loss:19.73 - Loss:19.734901146085306\n",
      " Iter:0 - Alpha:0.05 - Batch 214/249 - Min Loss:19.70 - Loss:19.70392836302221\n",
      " Iter:0 - Alpha:0.05 - Batch 215/249 - Min Loss:19.66 - Loss:19.662634289101913\n",
      " Iter:0 - Alpha:0.05 - Batch 216/249 - Min Loss:19.62 - Loss:19.62547395883272\n",
      " Iter:0 - Alpha:0.05 - Batch 217/249 - Min Loss:19.60 - Loss:19.600982714656038\n",
      " Iter:0 - Alpha:0.05 - Batch 218/249 - Min Loss:19.56 - Loss:19.560585333546253\n",
      " Iter:0 - Alpha:0.05 - Batch 219/249 - Min Loss:19.51 - Loss:19.510858793259793\n",
      " Iter:0 - Alpha:0.05 - Batch 220/249 - Min Loss:19.48 - Loss:19.48979081792196\n",
      " Iter:0 - Alpha:0.05 - Batch 221/249 - Min Loss:19.48 - Loss:19.484358637269835\n",
      " Iter:0 - Alpha:0.05 - Batch 222/249 - Min Loss:19.45 - Loss:19.457275961644104\n",
      " Iter:0 - Alpha:0.05 - Batch 223/249 - Min Loss:19.42 - Loss:19.427466004060804\n",
      " Iter:0 - Alpha:0.05 - Batch 224/249 - Min Loss:19.40 - Loss:19.401862985549656\n",
      " Iter:0 - Alpha:0.05 - Batch 225/249 - Min Loss:19.37 - Loss:19.371943160091128\n",
      " Iter:0 - Alpha:0.05 - Batch 226/249 - Min Loss:19.33 - Loss:19.336714280104292\n",
      " Iter:0 - Alpha:0.05 - Batch 227/249 - Min Loss:19.31 - Loss:19.31006555998877\n",
      " Iter:0 - Alpha:0.05 - Batch 228/249 - Min Loss:19.29 - Loss:19.297551594785666\n",
      " Iter:0 - Alpha:0.05 - Batch 229/249 - Min Loss:19.27 - Loss:19.270659505497946\n",
      " Iter:0 - Alpha:0.05 - Batch 230/249 - Min Loss:19.24 - Loss:19.244977190877837\n",
      " Iter:0 - Alpha:0.05 - Batch 231/249 - Min Loss:19.22 - Loss:19.229317641301492\n",
      " Iter:0 - Alpha:0.05 - Batch 232/249 - Min Loss:19.19 - Loss:19.198498239481133\n",
      " Iter:0 - Alpha:0.05 - Batch 233/249 - Min Loss:19.18 - Loss:19.18046325753652\n",
      " Iter:0 - Alpha:0.05 - Batch 234/249 - Min Loss:19.14 - Loss:19.149703730356013\n",
      " Iter:0 - Alpha:0.05 - Batch 235/249 - Min Loss:19.11 - Loss:19.116968929222868\n",
      " Iter:0 - Alpha:0.05 - Batch 236/249 - Min Loss:19.08 - Loss:19.08146279268546\n",
      " Iter:0 - Alpha:0.05 - Batch 237/249 - Min Loss:19.05 - Loss:19.05907982897195\n",
      " Iter:0 - Alpha:0.05 - Batch 238/249 - Min Loss:19.04 - Loss:19.041698333929684\n",
      " Iter:0 - Alpha:0.05 - Batch 239/249 - Min Loss:19.02 - Loss:19.02401255225597\n",
      " Iter:0 - Alpha:0.05 - Batch 240/249 - Min Loss:19.00 - Loss:19.003619000567706\n",
      " Iter:0 - Alpha:0.05 - Batch 241/249 - Min Loss:18.96 - Loss:18.966755715936493\n",
      " Iter:0 - Alpha:0.05 - Batch 242/249 - Min Loss:18.93 - Loss:18.93893808995805\n",
      " Iter:0 - Alpha:0.05 - Batch 244/249 - Min Loss:18.91 - Loss:18.918220781984296\n",
      " Iter:0 - Alpha:0.05 - Batch 245/249 - Min Loss:18.89 - Loss:18.898552281305815\n",
      " Iter:0 - Alpha:0.05 - Batch 246/249 - Min Loss:18.87 - Loss:18.87521833713537\n",
      " Iter:0 - Alpha:0.05 - Batch 247/249 - Min Loss:18.85 - Loss:18.852018065485318\n",
      " Iter:0 - Alpha:0.05 - Batch 248/249 - Min Loss:18.82 - Loss:18.826867426405627\n",
      " Iter:0 - Alpha:0.05 - Batch 249/249 - Min Loss:18.80 - Loss:18.80234813609365\n",
      " Iter:1 - Alpha:0.049 - Batch 1/249 - Min Loss:13.03 - Loss:13.030232292848563 - he the the ther ther ther ther ther ther ther ther ther ther ther ther\n",
      " Iter:1 - Alpha:0.049 - Batch 3/249 - Min Loss:12.87 - Loss:12.943347702693039\n",
      " Iter:1 - Alpha:0.049 - Batch 4/249 - Min Loss:12.80 - Loss:12.800310218278462\n",
      " Iter:2 - Alpha:0.049 - Batch 4/249 - Min Loss:12.71 - Loss:12.767640919540769 - hend therend therend therend therend therend therend therend therend the\n",
      " Iter:2 - Alpha:0.049 - Batch 249/249 - Min Loss:12.47 - Loss:12.504957906838602- herer, will wit will wit will wit will wit will wit will wit will wit e\n",
      " Iter:3 - Alpha:0.048 - Batch 1/249 - Min Loss:11.91 - Loss:11.915796551323828 - hend theres, Thend theres, Thend theres, Thend theres, Thend theres, T\n",
      " Iter:3 - Alpha:0.048 - Batch 4/249 - Min Loss:11.87 - Loss:11.946486986225331\n",
      " Iter:4 - Alpha:0.048 - Batch 18/249 - Min Loss:11.82 - Loss:11.831559591049132- hend theeseresteresteresteresteresteresteresteresteresteresteresterestil\n",
      " Iter:4 - Alpha:0.048 - Batch 20/249 - Min Loss:11.80 - Loss:11.864600488775306\n",
      " Iter:4 - Alpha:0.048 - Batch 23/249 - Min Loss:11.78 - Loss:11.837216123926497\n",
      " Iter:4 - Alpha:0.048 - Batch 24/249 - Min Loss:11.75 - Loss:11.756414843798407\n",
      " Iter:4 - Alpha:0.048 - Batch 25/249 - Min Loss:11.67 - Loss:11.677902477707049\n",
      " Iter:4 - Alpha:0.048 - Batch 26/249 - Min Loss:11.57 - Loss:11.576724214565099\n",
      " Iter:4 - Alpha:0.048 - Batch 34/249 - Min Loss:11.57 - Loss:11.604918538750152\n",
      " Iter:4 - Alpha:0.048 - Batch 35/249 - Min Loss:11.57 - Loss:11.57548847095236\n",
      " Iter:4 - Alpha:0.048 - Batch 37/249 - Min Loss:11.57 - Loss:11.574564767364791\n",
      " Iter:4 - Alpha:0.048 - Batch 55/249 - Min Loss:11.55 - Loss:11.572236193583972- h a me a the a weall as a we a lot a me a the a weall as a we a lot a \n",
      " Iter:4 - Alpha:0.048 - Batch 56/249 - Min Loss:11.53 - Loss:11.532972176693862\n",
      " Iter:4 - Alpha:0.048 - Batch 103/249 - Min Loss:11.50 - Loss:11.504070523569322- I the the the the the the the the the the the the the the the the the \n",
      " Iter:4 - Alpha:0.048 - Batch 104/249 - Min Loss:11.50 - Loss:11.500295000961874\n",
      " Iter:4 - Alpha:0.048 - Batch 105/249 - Min Loss:11.49 - Loss:11.49806236002666\n",
      " Iter:4 - Alpha:0.048 - Batch 106/249 - Min Loss:11.47 - Loss:11.479750978831477\n",
      " Iter:4 - Alpha:0.048 - Batch 107/249 - Min Loss:11.47 - Loss:11.479008739602122\n",
      " Iter:4 - Alpha:0.048 - Batch 126/249 - Min Loss:11.47 - Loss:11.475948253050793\n",
      " Iter:4 - Alpha:0.048 - Batch 127/249 - Min Loss:11.46 - Loss:11.46741844830866\n",
      " Iter:4 - Alpha:0.048 - Batch 128/249 - Min Loss:11.46 - Loss:11.460384607770528\n",
      " Iter:4 - Alpha:0.048 - Batch 129/249 - Min Loss:11.45 - Loss:11.454478700489087\n",
      " Iter:4 - Alpha:0.048 - Batch 130/249 - Min Loss:11.44 - Loss:11.447299083454329\n",
      " Iter:4 - Alpha:0.048 - Batch 131/249 - Min Loss:11.43 - Loss:11.43116118286202\n",
      " Iter:4 - Alpha:0.048 - Batch 132/249 - Min Loss:11.41 - Loss:11.415849059885435\n",
      " Iter:4 - Alpha:0.048 - Batch 133/249 - Min Loss:11.41 - Loss:11.415547786610968\n",
      " Iter:4 - Alpha:0.048 - Batch 135/249 - Min Loss:11.40 - Loss:11.413007450975094\n",
      " Iter:4 - Alpha:0.048 - Batch 137/249 - Min Loss:11.40 - Loss:11.403540873846929\n",
      " Iter:4 - Alpha:0.048 - Batch 138/249 - Min Loss:11.39 - Loss:11.390658699929363\n",
      " Iter:4 - Alpha:0.048 - Batch 140/249 - Min Loss:11.38 - Loss:11.384892076285892\n",
      " Iter:4 - Alpha:0.048 - Batch 141/249 - Min Loss:11.38 - Loss:11.382458199690419\n",
      " Iter:4 - Alpha:0.048 - Batch 142/249 - Min Loss:11.37 - Loss:11.379789041111076\n",
      " Iter:4 - Alpha:0.048 - Batch 153/249 - Min Loss:11.37 - Loss:11.382246849955552 - here, willest win theere, Whererererererererererererererererererererer\n",
      " Iter:4 - Alpha:0.048 - Batch 210/249 - Min Loss:11.37 - Loss:11.391900379098821 - hat willed willed willed willed willed willed willed willed willed wil\n",
      " Iter:4 - Alpha:0.048 - Batch 211/249 - Min Loss:11.37 - Loss:11.37312179010039\n",
      " Iter:4 - Alpha:0.048 - Batch 212/249 - Min Loss:11.36 - Loss:11.36649338101244\n",
      " Iter:4 - Alpha:0.048 - Batch 213/249 - Min Loss:11.35 - Loss:11.359428388284398\n",
      " Iter:4 - Alpha:0.048 - Batch 214/249 - Min Loss:11.35 - Loss:11.357698706043193\n",
      " Iter:4 - Alpha:0.048 - Batch 215/249 - Min Loss:11.34 - Loss:11.347191707486214\n",
      " Iter:4 - Alpha:0.048 - Batch 217/249 - Min Loss:11.33 - Loss:11.343178921534731\n",
      " Iter:4 - Alpha:0.048 - Batch 218/249 - Min Loss:11.33 - Loss:11.335314550603336\n",
      " Iter:4 - Alpha:0.048 - Batch 219/249 - Min Loss:11.32 - Loss:11.320586675988881\n",
      " Iter:4 - Alpha:0.048 - Batch 222/249 - Min Loss:11.31 - Loss:11.321410058307544\n",
      " Iter:4 - Alpha:0.048 - Batch 223/249 - Min Loss:11.31 - Loss:11.317430890968835\n",
      " Iter:4 - Alpha:0.048 - Batch 224/249 - Min Loss:11.31 - Loss:11.312439193662241\n",
      " Iter:4 - Alpha:0.048 - Batch 225/249 - Min Loss:11.30 - Loss:11.306708279231284\n",
      " Iter:4 - Alpha:0.048 - Batch 226/249 - Min Loss:11.30 - Loss:11.302876022468109\n",
      " Iter:4 - Alpha:0.048 - Batch 227/249 - Min Loss:11.29 - Loss:11.299835520039686\n",
      " Iter:5 - Alpha:0.047 - Batch 25/249 - Min Loss:11.29 - Loss:11.370195170487552- hend theend theend theend theend theend theend theend theend theend th\n",
      " Iter:5 - Alpha:0.047 - Batch 26/249 - Min Loss:11.25 - Loss:11.257859223342159\n",
      " Iter:5 - Alpha:0.047 - Batch 33/249 - Min Loss:11.24 - Loss:11.277797811307948\n",
      " Iter:5 - Alpha:0.047 - Batch 34/249 - Min Loss:11.23 - Loss:11.23527687907847\n",
      " Iter:5 - Alpha:0.047 - Batch 37/249 - Min Loss:11.21 - Loss:11.220561326929968\n",
      " Iter:5 - Alpha:0.047 - Batch 54/249 - Min Loss:11.19 - Loss:11.224365888478356- her sea there sea there sea there sea there sea there sea there sea th\n",
      " Iter:5 - Alpha:0.047 - Batch 55/249 - Min Loss:11.19 - Loss:11.191990974631079\n",
      " Iter:5 - Alpha:0.047 - Batch 56/249 - Min Loss:11.16 - Loss:11.160648619464311\n",
      " Iter:5 - Alpha:0.047 - Batch 94/249 - Min Loss:11.14 - Loss:11.148732534777011\n",
      " Iter:5 - Alpha:0.047 - Batch 96/249 - Min Loss:11.13 - Loss:11.135245381782982\n",
      " Iter:5 - Alpha:0.047 - Batch 97/249 - Min Loss:11.12 - Loss:11.125879425949742\n",
      " Iter:5 - Alpha:0.047 - Batch 100/249 - Min Loss:11.11 - Loss:11.112581740582897\n",
      " Iter:5 - Alpha:0.047 - Batch 101/249 - Min Loss:11.10 - Loss:11.100095908560776 - ure the the the the the the the the the the the the the the the the th\n",
      " Iter:5 - Alpha:0.047 - Batch 102/249 - Min Loss:11.08 - Loss:11.085058045497782\n",
      " Iter:5 - Alpha:0.047 - Batch 103/249 - Min Loss:11.08 - Loss:11.082975827871536\n",
      " Iter:5 - Alpha:0.047 - Batch 104/249 - Min Loss:11.07 - Loss:11.07774218825992\n",
      " Iter:5 - Alpha:0.047 - Batch 105/249 - Min Loss:11.07 - Loss:11.07530984427834\n",
      " Iter:5 - Alpha:0.047 - Batch 106/249 - Min Loss:11.05 - Loss:11.057692680401127\n",
      " Iter:5 - Alpha:0.047 - Batch 107/249 - Min Loss:11.05 - Loss:11.057214831668464\n",
      " Iter:5 - Alpha:0.047 - Batch 131/249 - Min Loss:11.05 - Loss:11.071432321791939\n",
      " Iter:5 - Alpha:0.047 - Batch 133/249 - Min Loss:11.05 - Loss:11.054318906398357\n",
      " Iter:5 - Alpha:0.047 - Batch 135/249 - Min Loss:11.04 - Loss:11.049193460961886\n",
      " Iter:5 - Alpha:0.047 - Batch 136/249 - Min Loss:11.03 - Loss:11.036283814022037\n",
      " Iter:5 - Alpha:0.047 - Batch 137/249 - Min Loss:11.03 - Loss:11.034528374631934\n",
      " Iter:5 - Alpha:0.047 - Batch 138/249 - Min Loss:11.01 - Loss:11.016536348611345\n",
      " Iter:5 - Alpha:0.047 - Batch 142/249 - Min Loss:11.00 - Loss:11.012708220299443\n",
      " Iter:5 - Alpha:0.047 - Batch 143/249 - Min Loss:11.00 - Loss:11.0071276911819\n",
      " Iter:5 - Alpha:0.047 - Batch 144/249 - Min Loss:11.00 - Loss:11.001705877998848\n",
      " Iter:5 - Alpha:0.047 - Batch 152/249 - Min Loss:10.99 - Loss:10.999793921574778 - here, willy there, willy there, willy there, willy there, willy there,\n",
      " Iter:5 - Alpha:0.047 - Batch 153/249 - Min Loss:10.99 - Loss:10.991932484678939\n",
      " Iter:5 - Alpha:0.047 - Batch 212/249 - Min Loss:10.98 - Loss:10.987041244501551 - hat willed That willed That willed That willed That willed That willed\n",
      " Iter:5 - Alpha:0.047 - Batch 213/249 - Min Loss:10.97 - Loss:10.979889236543452\n",
      " Iter:5 - Alpha:0.047 - Batch 214/249 - Min Loss:10.97 - Loss:10.97914732863416\n",
      " Iter:5 - Alpha:0.047 - Batch 215/249 - Min Loss:10.96 - Loss:10.968661109444282\n",
      " Iter:5 - Alpha:0.047 - Batch 217/249 - Min Loss:10.96 - Loss:10.966251226108422\n",
      " Iter:5 - Alpha:0.047 - Batch 218/249 - Min Loss:10.95 - Loss:10.959035878669576\n",
      " Iter:5 - Alpha:0.047 - Batch 219/249 - Min Loss:10.94 - Loss:10.944810171217446\n",
      " Iter:5 - Alpha:0.047 - Batch 221/249 - Min Loss:10.94 - Loss:10.947528956308501\n",
      " Iter:5 - Alpha:0.047 - Batch 222/249 - Min Loss:10.94 - Loss:10.943267829523139\n",
      " Iter:5 - Alpha:0.047 - Batch 223/249 - Min Loss:10.93 - Loss:10.938455021990597\n",
      " Iter:5 - Alpha:0.047 - Batch 224/249 - Min Loss:10.93 - Loss:10.93437982332902\n",
      " Iter:5 - Alpha:0.047 - Batch 225/249 - Min Loss:10.92 - Loss:10.928247980154639\n",
      " Iter:5 - Alpha:0.047 - Batch 226/249 - Min Loss:10.92 - Loss:10.924840395250811\n",
      " Iter:5 - Alpha:0.047 - Batch 227/249 - Min Loss:10.92 - Loss:10.922160224407902\n",
      " Iter:6 - Alpha:0.047 - Batch 56/249 - Min Loss:10.91 - Loss:10.933126668211178 - ure a where, Wother sere, What sere asere, What sere asere, What sere \n",
      " Iter:6 - Alpha:0.047 - Batch 58/249 - Min Loss:10.91 - Loss:10.914235025249106\n",
      " Iter:6 - Alpha:0.047 - Batch 96/249 - Min Loss:10.91 - Loss:10.914755714572228\n",
      " Iter:6 - Alpha:0.047 - Batch 97/249 - Min Loss:10.90 - Loss:10.907765681370561\n",
      " Iter:6 - Alpha:0.047 - Batch 99/249 - Min Loss:10.89 - Loss:10.900021969416182\n",
      " Iter:6 - Alpha:0.047 - Batch 100/249 - Min Loss:10.89 - Loss:10.894023641641347\n",
      " Iter:6 - Alpha:0.047 - Batch 101/249 - Min Loss:10.88 - Loss:10.881227268264668 - ure the the the the the the the the the the the the the the the the th\n",
      " Iter:6 - Alpha:0.047 - Batch 102/249 - Min Loss:10.86 - Loss:10.864258622155893\n",
      " Iter:6 - Alpha:0.047 - Batch 103/249 - Min Loss:10.86 - Loss:10.8623184965684\n",
      " Iter:6 - Alpha:0.047 - Batch 104/249 - Min Loss:10.85 - Loss:10.855555200435754\n",
      " Iter:6 - Alpha:0.047 - Batch 105/249 - Min Loss:10.85 - Loss:10.85324519619195\n",
      " Iter:6 - Alpha:0.047 - Batch 106/249 - Min Loss:10.83 - Loss:10.837025630913386\n",
      " Iter:6 - Alpha:0.047 - Batch 107/249 - Min Loss:10.83 - Loss:10.836685556038816\n",
      " Iter:6 - Alpha:0.047 - Batch 130/249 - Min Loss:10.83 - Loss:10.852065119396654\n",
      " Iter:6 - Alpha:0.047 - Batch 131/249 - Min Loss:10.83 - Loss:10.834692312280323\n",
      " Iter:6 - Alpha:0.047 - Batch 132/249 - Min Loss:10.81 - Loss:10.816472807355446\n",
      " Iter:6 - Alpha:0.047 - Batch 133/249 - Min Loss:10.81 - Loss:10.812579736118805\n",
      " Iter:6 - Alpha:0.047 - Batch 135/249 - Min Loss:10.80 - Loss:10.803781857762678\n",
      " Iter:6 - Alpha:0.047 - Batch 136/249 - Min Loss:10.79 - Loss:10.79214740517026\n",
      " Iter:6 - Alpha:0.047 - Batch 137/249 - Min Loss:10.79 - Loss:10.791393804953673\n",
      " Iter:6 - Alpha:0.047 - Batch 138/249 - Min Loss:10.77 - Loss:10.77456435213398\n",
      " Iter:6 - Alpha:0.047 - Batch 152/249 - Min Loss:10.76 - Loss:10.766801908110535 - here, and there, and there, and there, and there, and there, and there\n",
      " Iter:6 - Alpha:0.047 - Batch 153/249 - Min Loss:10.75 - Loss:10.759084596640635\n",
      " Iter:6 - Alpha:0.047 - Batch 154/249 - Min Loss:10.74 - Loss:10.747583328993487\n",
      " Iter:6 - Alpha:0.047 - Batch 207/249 - Min Loss:10.74 - Loss:10.749220861676127 - hat willed That willed That willed That willed That willed That willed\n",
      " Iter:6 - Alpha:0.047 - Batch 208/249 - Min Loss:10.74 - Loss:10.741378552806147\n",
      " Iter:6 - Alpha:0.047 - Batch 209/249 - Min Loss:10.73 - Loss:10.735178207039754\n",
      " Iter:6 - Alpha:0.047 - Batch 210/249 - Min Loss:10.73 - Loss:10.73045487712749\n",
      " Iter:6 - Alpha:0.047 - Batch 211/249 - Min Loss:10.71 - Loss:10.713275081654391\n",
      " Iter:6 - Alpha:0.047 - Batch 212/249 - Min Loss:10.70 - Loss:10.708401910490691\n",
      " Iter:6 - Alpha:0.047 - Batch 213/249 - Min Loss:10.69 - Loss:10.69985511790287\n",
      " Iter:6 - Alpha:0.047 - Batch 214/249 - Min Loss:10.69 - Loss:10.699501650157892\n",
      " Iter:6 - Alpha:0.047 - Batch 215/249 - Min Loss:10.68 - Loss:10.688519792717674\n",
      " Iter:6 - Alpha:0.047 - Batch 217/249 - Min Loss:10.68 - Loss:10.684279565758864\n",
      " Iter:6 - Alpha:0.047 - Batch 218/249 - Min Loss:10.67 - Loss:10.677031038808735\n",
      " Iter:6 - Alpha:0.047 - Batch 219/249 - Min Loss:10.66 - Loss:10.663330932194107\n",
      " Iter:6 - Alpha:0.047 - Batch 221/249 - Min Loss:10.66 - Loss:10.664220936911882\n",
      " Iter:6 - Alpha:0.047 - Batch 222/249 - Min Loss:10.65 - Loss:10.659819414475244\n",
      " Iter:6 - Alpha:0.047 - Batch 223/249 - Min Loss:10.65 - Loss:10.653995505893102\n",
      " Iter:6 - Alpha:0.047 - Batch 224/249 - Min Loss:10.65 - Loss:10.650152193440473\n",
      " Iter:6 - Alpha:0.047 - Batch 225/249 - Min Loss:10.64 - Loss:10.645211303941233\n",
      " Iter:6 - Alpha:0.047 - Batch 226/249 - Min Loss:10.64 - Loss:10.642050721561775\n",
      " Iter:6 - Alpha:0.047 - Batch 227/249 - Min Loss:10.63 - Loss:10.638429235190879\n",
      " Iter:6 - Alpha:0.047 - Batch 236/249 - Min Loss:10.63 - Loss:10.636717477943202\n",
      " Iter:7 - Alpha:0.046 - Batch 1/249 - Min Loss:10.63 - Loss:11.08365964500645 - heres, and seesees theest ates theest ates theest ates theest ates the\n",
      " Iter:7 - Alpha:0.046 - Batch 2/249 - Min Loss:10.53 - Loss:10.537889115661347\n",
      " Iter:7 - Alpha:0.046 - Batch 236/249 - Min Loss:10.42 - Loss:10.428290250061355- hat willed That willed That willed That willed That willed That willed \n",
      " Iter:7 - Alpha:0.046 - Batch 237/249 - Min Loss:10.42 - Loss:10.42170483677893\n",
      " Iter:7 - Alpha:0.046 - Batch 242/249 - Min Loss:10.42 - Loss:10.424925861494176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter:8 - Alpha:0.046 - Batch 1/249 - Min Loss:10.42 - Loss:10.919361588277546 - her Theest ates, and ther Theest ates, and ther Theest ates, and ther \n",
      " Iter:8 - Alpha:0.046 - Batch 2/249 - Min Loss:10.37 - Loss:10.37168008432074\n",
      " Iter:8 - Alpha:0.046 - Batch 4/249 - Min Loss:10.34 - Loss:10.377784596040074\n",
      " Iter:9 - Alpha:0.045 - Batch 4/249 - Min Loss:10.25 - Loss:10.375282845952936 - her, and ther ther ther ther ther ther ther ther ther ther ther ther t w\n",
      " Iter:10 - Alpha:0.045 - Batch 1/249 - Min Loss:10.21 - Loss:10.457589321108864 - her then then then then then then then then then then then then then tw\n",
      " Iter:10 - Alpha:0.045 - Batch 2/249 - Min Loss:10.01 - Loss:10.017504429045356\n",
      " Iter:15 - Alpha:0.043 - Batch 1/249 - Min Loss:10.00 - Loss:10.046528196004962 - he seell deer Then men eneer then seer then seer then seer then seer t, \n",
      " Iter:15 - Alpha:0.043 - Batch 249/249 - Min Loss:9.887 - Loss:10.393878428499486 - hat will will will will will will will will will will will will will w\n",
      " Iter:16 - Alpha:0.042 - Batch 1/249 - Min Loss:9.878 - Loss:9.87853848371361 - hesenter That them seell deen een een een een een een een een een een \n",
      " Iter:54 - Alpha:0.029 - Batch 213/249 - Min Loss:9.867 - Loss:15.030857306711082 - ithy, wifownd.  That will stred That will stred That will stred That w"
     ]
    }
   ],
   "source": [
    "train(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32adc964",
   "metadata": {},
   "source": [
    "Thx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93660dd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
